# -*- coding: utf-8 -*-
# --------------------------------------
# @Time    : 2019/10/18 14:40
# @Author  : hxf
# @Email   : 1870212598@qq.com
# @File    : wenben2.py
# Description : 计算文本相似度
# ----------------------------------
'''
计算文本相似度
'''
from gensim import corpora,models,similarities
import jieba
from collections import defaultdict
doc1="data/d1.txt"
doc2="data/d2.txt"
# 1、加载文档
d1=open(doc1,encoding='UTF-8').read()
d2=open(doc2,encoding='UTF-8').read()
# 2、进行分词
data1=jieba.cut(d1)
data2=jieba.cut(d2)
'''
for item in data1:
	print(item)
print("")
for item in data1:
	print(item)
'''
#"词语1 词语2 词语3 … 词语n"
# 3、整理文档
data11=""
for item in data1:
    data11+=item+" "
# print(data11)
data21=""
for item in data2:
    data21+=item+" "
documents=[data11,data21]#放入文档

texts=[[word for word in document.split()]
       for document in documents]
# print(texts)

# 4、计算出词语的频率
frequency=defaultdict(int)
for text in texts:
    for token in text:
        frequency[token]+=1
# print(frequency)
'''defaultdict(<class 'int'>, {'2011': 4, '年': 17, '9': 7, '月': 17, '成立': 3, '于': 2, '上海': 3, '。': 23, '，': 69, '天善': 26, '智能': 17, '由': 2, '3': 2, '位': 4, '从事': 1, '商业智能': 10, '工作': 2, '的': 29, '技术人员': 1, '组织': 1, '每位': 1, '成员': 4, '都': 1, '非常': 2, '热爱': 1, '技术': 8, '并且': 4, '看好': 1, '在': 6, '中国': 2, '发展': 2, '巨大': 1, '潜力': 1, '从': 1, '最初': 1, '人': 12, '至今': 1, '已经': 4, '拥有': 1, '15': 2, '如今': 1, '成为': 1, '最': 3, '受欢迎': 1, '影响力': 1, '最大': 1, '团队': 2, '至': 1, '2012': 10, '为': 2, '一些': 2, '中小型': 1, '公司': 4, '提供': 2, '技术支持': 1, '以及': 2, '培训': 10, '服务': 3, '受': 2, '了': 14, '客户': 3, '一致': 1, '好评': 2, '通过': 6, '推荐': 1, '口碑': 3, '传播': 3, '越来越': 1, '多': 2, '和': 7, '个人': 1, '希望': 2, '能': 1, '技术培训': 2, '帮助': 2, '增加': 2, '人才': 2, '储备': 1, '正是': 1, '因为': 2, '看到': 1, '这样': 3, '一个': 1, '需求': 1, '2': 4, '举行': 3, '首次': 6, '公开课': 8, '吸纳': 1, '进行': 1, '直至': 1, '2013': 3, '1': 2, '成功': 2, '举办': 3, '4': 2, '期': 1, '线下': 2, '期线': 1, '上': 3, '开始': 1, '网络': 8, '来': 3, '多年': 1, '数据库': 1, '方面': 1, '实战经验': 1, '目前': 1, '总共': 1, '制作': 1, '23': 1, '份': 1, '文档': 1, '录制': 1, '25': 1, '部': 1, '视频教程': 1, '（': 1, '陆续': 1, '中': 1, '...': 1, '）': 1, '开展': 2, '六期': 1, 'BI': 11, '把': 1, '这': 1, '资料': 2, '放在': 1, '官方': 2, '博客': 2, '优酷': 1, '各种': 1, '网络媒介': 1, '免费': 2, '让': 1, '大家': 2, '下载': 1, '观看': 1, '学习': 3, '难得': 1, '是': 3, '这些': 2, '独有': 1, '最全': 1, '最有': 1, '含金量': 1, '整理': 1, '井井有序': 1, '开放': 1, '给': 1, '做': 2, '目的': 1, '完全': 1, '是因为': 1, '体谅': 1, '大学生': 1, '、': 2, '初学者': 2, '想': 2, '但是': 1, '又': 1, '苦于': 1, '无门': 1, '心情': 1, '所有': 1, '也': 2, '经历': 2, '过': 1, '一段': 1, '痛苦': 1, '时间': 1, '因此': 2, '小小的': 1, '行为': 2, '到': 1, '那些': 1, '完成': 1, '招生': 1, '课程': 2, '分别': 1, '微软': 4, '解决方案': 1, 'Cognos': 4, '其中': 1, '13': 2, '10': 2, 'YY': 1, '教育': 1, '平台': 1, '参与': 2, '听课': 3, '人数': 8, '多达': 2, '190': 1, '广大': 1, '同仁': 2, '一直': 1, '12': 2, '创办者': 1, '金牌': 1, '讲师': 1, '梁勇': 1, '主讲': 1, '内容': 1, '超过': 2, '150': 1, '就': 1, '有': 2, '如此': 1, '支持': 2, '离不开': 1, '平时': 1, '乐于助人': 1, '11': 1, '现场': 4, '80': 1, '外地': 1, '朋友': 2, '赶来': 1, '气氛': 1, '火爆': 1, '反响': 1, '好': 1, '众多': 1, '提出': 1, '我们': 1, '能够': 1, '以': 1, '解决': 1, '地域': 1, '限制': 1, '问题': 1, '同时': 1, '本月底': 1, '第四期': 1, '学员': 5, '毕业': 8, '前': 2, '四期': 1, '总计': 1, '35': 1, '转岗': 1, '28': 1, '经过': 1, '几期': 1, '建立': 2, '良好': 1, '决定': 1, '自己': 1, '随后': 1, '官方网站': 1, '8': 2, '第三期': 1, '6': 1, '第二期': 1, '第一期': 1, '20': 1, '爱好者': 1, '技术实力': 1, '其他': 1, '技术顾问': 1})
'''
'''
# 5、对频率低的词语进行过滤
texts=[[word for word in text if frequency[token]>3]
 for text in texts]
print(texts)
'''
# 6、通过语料库建立词典
dictionary=corpora.Dictionary(texts)
dictionary.save("data/d345.txt")
# 7、加载对比文档
doc3="data/d3.txt"
d3=open(doc3,encoding="UTF-8").read()
data3=jieba.cut(d3)
data31=""
for item in data3:
    data31+=item+" "
# print(data31)
# 8、将要对比的文档通过doc2bow转化为稀疏向量
new_doc=data31
new_vec=dictionary.doc2bow(new_doc.split())
# 9、对稀疏向量进行进一步处理，得到新语料库
corpus=[dictionary.doc2bow(text) for text in texts]
# print(corpus)
corpora.MmCorpus.serialize("data/d3.mm",corpus)
# 10、将新语料库通过tfidfmodel进行处理，得到tfidf
tfidf=models.TfidfModel(corpus)
# 11、通过token2id得到特征数
featureNum=len(dictionary.token2id.keys())
print(featureNum)
# 12、稀疏矩阵相似度，从而建立索引
index=similarities.SparseMatrixSimilarity(tfidf[corpus],num_features=featureNum)
sim=index[tfidf[new_vec]]
print(sim)
'''
[0.51968324 0.        ]
'''